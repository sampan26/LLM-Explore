{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch accelerate huggingface-hub huggingface-cli hf-transfer\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 10**9\n",
    "    print(f\"Model Size:{num_params:.3f}B parameters\")\n",
    "    return int(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Load meta-llama/Meta-Llama-3-8B model, config and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_weigths(reference_model, n_layers):\n",
    "    params = {}\n",
    "    cur_layer = 0\n",
    "    \n",
    "    for name, module in reference_model.named_modules():\n",
    "        if isinstance(module, torch.nn.Module):\n",
    "            layer_match = re.match(r'model\\.layers\\.(\\d+)', name)\n",
    "            if layer_match and int(layer_match.group(1)) >= n_layers:\n",
    "                break\n",
    "\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "                params[f'{name}.weight'] = module.weight.data.clone()\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            params[f'{name}.bias'] = module.bias.data.clone()\n",
    "    \n",
    "    norm_layer = model.model.norm\n",
    "    if hasattr(norm_layer, 'weight') and norm_layer.weight is not None:\n",
    "        params['model.norm.weight'] = norm_layer.weight.data.clone()\n",
    "    if hasattr(norm_layer, 'bias') and norm_layer.bias is not None:\n",
    "        params['model.norm.bias'] = norm_layer.bias.data.clone()\n",
    "\n",
    "    lm_head = reference_model.lm_head\n",
    "    if hasattr(lm_head, 'weight') and lm_head.weight is not None:\n",
    "        params[\"lm_head.weight\"] = lm_head.weight.data\n",
    "    if hasattr(lm_head, 'bias') and lm_head.bias is not None:\n",
    "        params[\"lm_head.bias\"] = lm_head.bias.data\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_n_layers = 24\n",
    "pretrained_weights = extract_model_weights(model, target_model_n_layers)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
